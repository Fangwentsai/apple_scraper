#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼·ç‰ˆ Apple æ•´ä¿®å“çˆ¬èŸ²ç¨‹å¼
å°ˆé–€æå–ç”¢å“è©³ç´°è¦æ ¼å’Œæ¦‚è¦½è³‡è¨Š
"""

import asyncio
import json
import re
import os
from datetime import datetime
from playwright.async_api import async_playwright
import logging
from typing import Dict, List, Optional
import time

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnhancedAppleScraper:
    def __init__(self):
        """åˆå§‹åŒ–å¢å¼·ç‰ˆçˆ¬èŸ²"""
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0'
        }
        
        self.categories = {
            'mac': 'https://www.apple.com/tw/shop/refurbished/mac',
            'ipad': 'https://www.apple.com/tw/shop/refurbished/ipad',
            'iphone': 'https://www.apple.com/tw/shop/refurbished/iphone',
            'airpods': 'https://www.apple.com/tw/shop/refurbished/airpods',
            'homepod': 'https://www.apple.com/tw/shop/refurbished/homepod',
            'appletv': 'https://www.apple.com/tw/shop/refurbished/appletv',
            'accessories': 'https://www.apple.com/tw/shop/refurbished/accessories'
        }

    async def setup_browser_context(self, browser):
        """è¨­å®šç€è¦½å™¨ä¸Šä¸‹æ–‡"""
        context = await browser.new_context(
            user_agent=self.headers['User-Agent'],
            extra_http_headers=self.headers,
            viewport={'width': 1920, 'height': 1080},
            locale='zh-TW',
            timezone_id='Asia/Taipei'
        )
        
        await context.set_geolocation({'latitude': 25.0330, 'longitude': 121.5654})
        return context

    async def extract_product_overview(self, page, product_url: str) -> str:
        """å¾ç”¢å“è©³ç´°é é¢æå–å®Œæ•´çš„ç”¢å“æ¦‚è¦½"""
        try:
            logger.info(f"ğŸ” æå–ç”¢å“æ¦‚è¦½: {product_url}")
            
            # è¨ªå•ç”¢å“è©³ç´°é é¢
            await page.goto(product_url, wait_until='networkidle', timeout=30000)
            await asyncio.sleep(2)
            
            # å˜—è©¦å¤šç¨®é¸æ“‡å™¨ä¾†æ‰¾åˆ°ç”¢å“æ¦‚è¦½å€åŸŸ
            overview_selectors = [
                '.rc-pdsection-mainpanel.column.large-9.small-12',
                '.rc-pdsection-mainpanel',
                '.pd-overview',
                '.pd-highlights',
                '[data-module-template="pd/overview"]',
                '.pd-overview-content',
                '.rf-pdp-overview'
            ]
            
            overview_text = ""
            
            for selector in overview_selectors:
                try:
                    elements = await page.query_selector_all(selector)
                    if elements:
                        logger.info(f"âœ… æ‰¾åˆ°æ¦‚è¦½å€åŸŸ: {selector}")
                        
                        for element in elements:
                            # æå–æ–‡å­—å…§å®¹
                            text = await element.inner_text()
                            if text and len(text.strip()) > 50:  # ç¢ºä¿æœ‰è¶³å¤ çš„å…§å®¹
                                overview_text += text.strip() + "\n\n"
                        
                        if overview_text:
                            break
                            
                except Exception as e:
                    logger.debug(f"é¸æ“‡å™¨ {selector} å¤±æ•—: {e}")
                    continue
            
            # å¦‚æœæ²’æœ‰æ‰¾åˆ°æ¦‚è¦½å€åŸŸï¼Œå˜—è©¦æå–ç”¢å“è¦æ ¼
            if not overview_text:
                logger.info("ğŸ”„ å˜—è©¦æå–ç”¢å“è¦æ ¼...")
                overview_text = await self.extract_product_specs(page)
            
            # æ¸…ç†å’Œæ ¼å¼åŒ–æ–‡å­—
            if overview_text:
                overview_text = self.clean_overview_text(overview_text)
                logger.info(f"âœ… æˆåŠŸæå–æ¦‚è¦½ ({len(overview_text)} å­—å…ƒ)")
                return overview_text
            else:
                logger.warning("âš ï¸ æœªèƒ½æå–åˆ°ç”¢å“æ¦‚è¦½")
                return ""
                
        except Exception as e:
            logger.error(f"âŒ æå–ç”¢å“æ¦‚è¦½å¤±æ•—: {e}")
            return ""

    async def extract_product_specs(self, page) -> str:
        """æå–ç”¢å“è¦æ ¼è³‡è¨Š"""
        specs_text = ""
        
        # å˜—è©¦å¤šç¨®è¦æ ¼é¸æ“‡å™¨
        spec_selectors = [
            '.rf-pdp-techspecs',
            '.pd-techspecs',
            '.rf-pdp-highlights',
            '.pd-highlights',
            '.rf-pdp-overview-content',
            '.pd-overview-content',
            '[data-module-template="pd/techspecs"]',
            '.rf-pdp-content'
        ]
        
        for selector in spec_selectors:
            try:
                elements = await page.query_selector_all(selector)
                if elements:
                    logger.info(f"âœ… æ‰¾åˆ°è¦æ ¼å€åŸŸ: {selector}")
                    
                    for element in elements:
                        text = await element.inner_text()
                        if text and len(text.strip()) > 30:
                            specs_text += text.strip() + "\n\n"
                    
                    if specs_text:
                        break
                        
            except Exception as e:
                logger.debug(f"è¦æ ¼é¸æ“‡å™¨ {selector} å¤±æ•—: {e}")
                continue
        
        return specs_text

    def clean_overview_text(self, text: str) -> str:
        """æ¸…ç†å’Œæ ¼å¼åŒ–æ¦‚è¦½æ–‡å­—"""
        if not text:
            return ""
        
        # ç§»é™¤å¤šé¤˜çš„ç©ºç™½å’Œæ›è¡Œ
        text = re.sub(r'\n\s*\n', '\n', text)
        text = re.sub(r'\s+', ' ', text)
        
        # ç§»é™¤ä¸éœ€è¦çš„å…§å®¹
        unwanted_patterns = [
            r'åŠ å…¥è³¼ç‰©è»Š.*',
            r'ç«‹å³è³¼è²·.*',
            r'é¸æ“‡.*',
            r'Cookie.*',
            r'éš±ç§æ¬Š.*',
            r'ä½¿ç”¨æ¢æ¬¾.*',
            r'Â©.*Apple.*',
            r'å°ç£.*'
        ]
        
        for pattern in unwanted_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # é™åˆ¶é•·åº¦
        if len(text) > 1000:
            text = text[:1000] + "..."
        
        return text.strip()

    async def scrape_category_with_details(self, category: str, limit: int = 5) -> List[Dict]:
        """çˆ¬å–æŒ‡å®šé¡åˆ¥çš„ç”¢å“ä¸¦æå–è©³ç´°æ¦‚è¦½"""
        logger.info(f"ğŸš€ é–‹å§‹çˆ¬å– {category.upper()} é¡åˆ¥çš„è©³ç´°è³‡è¨Š...")
        
        products = []
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=False,
                args=[
                    '--no-sandbox',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-web-security'
                ]
            )
            
            context = await self.setup_browser_context(browser)
            page = await context.new_page()
            
            try:
                # è¨ªå•é¡åˆ¥é é¢
                category_url = self.categories.get(category)
                if not category_url:
                    logger.error(f"âŒ æœªçŸ¥é¡åˆ¥: {category}")
                    return products
                
                logger.info(f"ğŸ“± è¨ªå•é¡åˆ¥é é¢: {category_url}")
                await page.goto(category_url, wait_until='networkidle', timeout=60000)
                await asyncio.sleep(3)
                
                # å°‹æ‰¾ç”¢å“é€£çµ
                product_selectors = [
                    'a[href*="/product/"]',
                    '.rf-serp-productcard a',
                    '.rf-serp-productcard-link',
                    '[data-testid="product-tile"] a'
                ]
                
                product_links = []
                
                for selector in product_selectors:
                    try:
                        elements = await page.query_selector_all(selector)
                        if elements:
                            logger.info(f"âœ… æ‰¾åˆ° {len(elements)} å€‹ç”¢å“é€£çµ (é¸æ“‡å™¨: {selector})")
                            
                            for element in elements[:limit]:  # é™åˆ¶æ•¸é‡
                                href = await element.get_attribute('href')
                                if href and '/product/' in href:
                                    if not href.startswith('http'):
                                        href = 'https://www.apple.com' + href
                                    product_links.append(href)
                            
                            break
                            
                    except Exception as e:
                        logger.debug(f"ç”¢å“é€£çµé¸æ“‡å™¨ {selector} å¤±æ•—: {e}")
                        continue
                
                if not product_links:
                    logger.warning(f"âš ï¸ æœªæ‰¾åˆ° {category} é¡åˆ¥çš„ç”¢å“é€£çµ")
                    return products
                
                logger.info(f"ğŸ”— æ‰¾åˆ° {len(product_links)} å€‹ç”¢å“é€£çµ")
                
                # æå–æ¯å€‹ç”¢å“çš„è©³ç´°è³‡è¨Š
                for i, product_url in enumerate(product_links, 1):
                    try:
                        logger.info(f"ğŸ“¦ è™•ç†ç”¢å“ {i}/{len(product_links)}")
                        
                        # æå–åŸºæœ¬ç”¢å“è³‡è¨Š
                        product_info = await self.extract_basic_product_info(page, product_url)
                        
                        # æå–è©³ç´°æ¦‚è¦½
                        detailed_overview = await self.extract_product_overview(page, product_url)
                        
                        if product_info:
                            product_info['ç”¢å“æ¦‚è¦½'] = detailed_overview or product_info.get('ç”¢å“æ¨™é¡Œ', '')
                            product_info['category'] = category
                            product_info['åºè™Ÿ'] = i
                            products.append(product_info)
                            
                            logger.info(f"âœ… æˆåŠŸè™•ç†ç”¢å“: {product_info.get('ç”¢å“æ¨™é¡Œ', '')[:50]}...")
                        
                        # é¿å…è¢«å°é–
                        await asyncio.sleep(2)
                        
                    except Exception as e:
                        logger.error(f"âŒ è™•ç†ç”¢å“å¤±æ•— {product_url}: {e}")
                        continue
                
            except Exception as e:
                logger.error(f"âŒ çˆ¬å–é¡åˆ¥ {category} å¤±æ•—: {e}")
            finally:
                await page.close()
                await context.close()
                await browser.close()
        
        logger.info(f"ğŸ‰ {category.upper()} é¡åˆ¥çˆ¬å–å®Œæˆï¼Œå…± {len(products)} å€‹ç”¢å“")
        return products

    async def extract_basic_product_info(self, page, product_url: str) -> Optional[Dict]:
        """æå–åŸºæœ¬ç”¢å“è³‡è¨Š"""
        try:
            await page.goto(product_url, wait_until='networkidle', timeout=30000)
            await asyncio.sleep(1)
            
            # æå–ç”¢å“æ¨™é¡Œ
            title_selectors = [
                'h1.rf-pdp-title',
                'h1[data-autom="pdp-product-name"]',
                '.rf-pdp-title',
                'h1.pd-title',
                'h1'
            ]
            
            title = ""
            for selector in title_selectors:
                try:
                    element = await page.query_selector(selector)
                    if element:
                        title = await element.inner_text()
                        if title:
                            break
                except:
                    continue
            
            # æå–åƒ¹æ ¼
            price_selectors = [
                '.rf-pdp-price',
                '.pd-price',
                '[data-autom="price"]',
                '.price'
            ]
            
            price = ""
            for selector in price_selectors:
                try:
                    element = await page.query_selector(selector)
                    if element:
                        price = await element.inner_text()
                        if 'NT$' in price:
                            break
                except:
                    continue
            
            if title:
                return {
                    'ç”¢å“æ¨™é¡Œ': title.strip(),
                    'ç”¢å“å”®åƒ¹': price.strip() if price else 'N/A',
                    'ç”¢å“URL': product_url
                }
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ æå–åŸºæœ¬ç”¢å“è³‡è¨Šå¤±æ•—: {e}")
            return None

    def save_enhanced_products(self, products: List[Dict], category: str):
        """å„²å­˜å¢å¼·ç‰ˆç”¢å“è³‡æ–™"""
        if not products:
            logger.warning(f"âš ï¸ {category} é¡åˆ¥æ²’æœ‰ç”¢å“è³‡æ–™å¯å„²å­˜")
            return
        
        try:
            # ç¢ºä¿ data ç›®éŒ„å­˜åœ¨
            os.makedirs('data', exist_ok=True)
            
            # å„²å­˜åˆ°å°æ‡‰çš„æª”æ¡ˆ
            filename = f'data/apple_refurbished_{category}_enhanced.json'
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(products, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ {category.upper()} å¢å¼·ç‰ˆè³‡æ–™å·²å„²å­˜åˆ° {filename}")
            logger.info(f"ğŸ“Š å…±å„²å­˜ {len(products)} å€‹ç”¢å“")
            
            # é¡¯ç¤ºç¯„ä¾‹
            if products:
                sample = products[0]
                logger.info(f"ğŸ“ ç¯„ä¾‹ç”¢å“æ¦‚è¦½:")
                logger.info(f"   æ¨™é¡Œ: {sample.get('ç”¢å“æ¨™é¡Œ', '')[:50]}...")
                logger.info(f"   æ¦‚è¦½: {sample.get('ç”¢å“æ¦‚è¦½', '')[:100]}...")
            
        except Exception as e:
            logger.error(f"âŒ å„²å­˜ç”¢å“è³‡æ–™å¤±æ•—: {e}")

async def main():
    """ä¸»ç¨‹å¼"""
    print("ğŸ å¢å¼·ç‰ˆ Apple æ•´ä¿®å“çˆ¬èŸ²ç¨‹å¼")
    print("å°ˆé–€æå–ç”¢å“è©³ç´°è¦æ ¼å’Œæ¦‚è¦½è³‡è¨Š")
    print("=" * 60)
    
    scraper = EnhancedAppleScraper()
    
    # é¸æ“‡è¦çˆ¬å–çš„é¡åˆ¥
    print("\nå¯ç”¨é¡åˆ¥:")
    for i, category in enumerate(scraper.categories.keys(), 1):
        print(f"{i}. {category.upper()}")
    
    try:
        choice = input("\nè«‹é¸æ“‡é¡åˆ¥ (è¼¸å…¥æ•¸å­—ï¼Œæˆ–æŒ‰ Enter çˆ¬å– Mac): ").strip()
        
        if not choice:
            category = 'mac'
        else:
            categories = list(scraper.categories.keys())
            category = categories[int(choice) - 1]
        
        limit = input(f"\nè«‹è¼¸å…¥è¦çˆ¬å–çš„ç”¢å“æ•¸é‡ (é è¨­ 5): ").strip()
        limit = int(limit) if limit.isdigit() else 5
        
        print(f"\nğŸš€ é–‹å§‹çˆ¬å– {category.upper()} é¡åˆ¥ï¼Œé™åˆ¶ {limit} å€‹ç”¢å“...")
        
        # åŸ·è¡Œçˆ¬å–
        products = await scraper.scrape_category_with_details(category, limit)
        
        if products:
            # å„²å­˜çµæœ
            scraper.save_enhanced_products(products, category)
            
            print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")
            print(f"ğŸ“Š æˆåŠŸçˆ¬å– {len(products)} å€‹ç”¢å“")
            print(f"ğŸ’¾ è³‡æ–™å·²å„²å­˜åˆ° data/apple_refurbished_{category}_enhanced.json")
        else:
            print(f"\nâŒ æœªèƒ½çˆ¬å–åˆ°ä»»ä½•ç”¢å“è³‡æ–™")
    
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ ä½¿ç”¨è€…ä¸­æ–·ç¨‹å¼")
    except Exception as e:
        print(f"\nâŒ ç¨‹å¼åŸ·è¡ŒéŒ¯èª¤: {e}")

if __name__ == "__main__":
    asyncio.run(main()) 